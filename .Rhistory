glm.prob = predict(glm.fit, data=Default, subset=-train)
glm.prob = predict(glm.fit, data=Default, subset=!train)
train=sample(nrow(Default), nrow(Default)/2)
train=sample(nrow(Default), (nrow(Default)/2))
source("dataprep.R")
source("dataprep.R")
setwd("~/GitHub/breast-cancer-classification.R")
source("dataprep.R")
brcancer = dataprep()
#divide the dataset into training and testing samples
require(caTools)
set.seed(1)
train = sample.split(brcancer$ID, SplitRatio = 0.75)
attach(brcancer)
glm.fit = glm(Diag~.-ID, data=brcancer, subset=train, family=binomial) # we don't count on ID as a predictor
# I got warning messages about the algorithm did not converge
# Apparently the fitted probabilities are extremely close to zero or one.
summary(glm.fit)
#Pay attention that there is no variable showing significant z-statistic test.
# At least, this method will not be that accurate.
glm.prob = predict(glm.fit, brcancer[!train,], type="response")
glm.prob = predict(glm.fit, default[!train], type="response")
glm.fit = glm(default~balance+income, data=Default, family = binomial, subset=train)
glm.prob = predict(glm.fit, default[!train], type="response")
glm.prob = predict(glm.fit, default[!train,], type="response")
glm.prob = predict(glm.fit, Default[!train,], type="response")
glm.default = ifelse(glm.prob>0.5, "Yes", "No")
mean(glm.default != default[!train])
nrow(Default[1])
nrow(default[!train])
train = sample(dim(Default)[1], dim(Default)[1]/2)
glm.fit = glm(default~balance+income, data=Default, family = binomial, subset=train)
glm.prob = predict(glm.fit, Default[!train,], type="response")
glm.prob = predict(glm.fit, Default[!train,], type="response")
rm(list=ls())
clear
clear()
library(ISLR)
summary(Default)
dim(Default)
attach(Default)
glm.fit = glm(default~income+balance, data=Default, family=binomial)
summary(glm.fit)
train = sample(nrow(default), nrow(default)/2)
train = sample(nrow(default), floor(nrow(default)/2))
nrow(default)
train = sample(dim(Default)[1], dim(Default)[1]/2)
glm.fit = glm(default~income+balance, data=Default, subset=train, family=binomial)
glm.prob = predict(glm.fit, Default[!train,], type="response")
glm.prob = predict(glm.fit, Default[-train,], type="response")
glm.pred = ifelse(glm.prob>0.5, "Yes", "No")
table(glm.pred, default[-train])
table(glm.pred, default[!train])
table(glm.pred, default[!train])
table(glm.pred, default[-train])
mean(glm.pred != default[-train])
contrast(default)
contrasts(default)
glm.fit = glm(default~income+balance+income, data=Default, subset=train, family=binomial)
glm.prob = predict(glm.fit, Default[-train,], type="response")
glm.pred = ifelse(glm.prob>0.5, "Yes", "No")
mean(glm.pred != default[-train])
glm.fit = glm(default~income+balance+student, data=Default, subset=train, family=binomial)
glm.prob = predict(glm.fit, Default[-train,], type="response")
glm.pred = ifelse(glm.prob>0.5, "Yes", "No")
mean(glm.pred != default[-train])
glm.fit = glm(default~income+balance, data=Default, subset=train, family=binomial)
coefs(glm.fit)
coef(glm.fit)
summary(glm.fit)
boot.fn() = function() {}
boot.fn() = function() {
}
boot.fn() = function(data, index) {
glm(default~income+balance, data=Default, family=binomial)$coefs
}
glm.fit$coefficients
glm.fit$coefs
boot.fn() = function(data, index) {
glm(default~income+balance, data=Default, family=binomial)$coeficients
}
boot.fn() = function(data, index) {
glm(default~income+balance, data=data, family=binomial)$coeficients
}
b
b
boot.fn() = function(data, index) {
glm(default~income+balance, data=data, family=binomial)$coeficients
}
glm(default~income+balance, data=Default, family=binomial)$coeficients
glm.fit$coefficients
glm.fit=glm(default~income+balance, data=Default, family=binomial)
glm.fit$coefficients
glm(default~income+balance, data=Default, family=binomial)$coefficients
glm(default~income+balance, data=Default, family=binomial)$coefficients
boot.fn() = function(data, index) {
glm(default~income+balance, data=data, family=binomial)$coefficients
}
boot.fn() = function(data, index) {
glm(default~income+balance, data=data, family=binomial, subset=index)$coefficients
}
boot.fn  = function(data, index) {
glm(default~income+balance, data=data, family=binomial, subset=index)$coefficients
}
boot.fn(Default, 1:5000)
boot(Default, boot.fn, 1000)
library(boot) # for the boot() function (bootstrap analysis)
boot(Default, boot.fn, 1000)
summary(glm(default~income+balance, subset=index, family=binomial))
summary(glm(default~income+balance, family=binomial))
cv.glm?
}
?cv.glm
?Weekly
rm(list=ls())
attach(Weekly)
glm.fit = glm(Direction~Lag1+Lag2, data = Weekly, family=binomial)
glm.fit = glm(Direction~Lag1+Lag2, data = Weekly[-1,], family=binomial)
glm.fit1 = glm(Direction~Lag1+Lag2, data = Weekly, family=binomial)
glm.fit2 = glm(Direction~Lag1+Lag2, data = Weekly[-1,], family=binomial)
summary(glm.fit1)
summary(glm.fit2)
pred1 = predict(glm.fit1, Weekly[1,], type="response")
prob1 = pred1
pred1 = ifelse(pro1>0.5, "Up", "Down")
pred1 = ifelse(prob1>0.5, "Up", "Down")
pred1
pred1 == Direction[1]
predict(glm.fit1, Weekly[1,], type="response")
predict.glm(glm.fit1, Weekly[1,], type="response")
?predict.glm
library(boot) # for the boot() function (bootstrap analysis)
boot.fn  = function(data, index) {
glm(default~income+balance, data=data, family=binomial, subset=index)$coefficients
}
boot(Default, boot.fn, 10)
?Boston
library(MASS) # for the Boston's housing data
?Boston
summary(Boston$medv)
mean(medv)
attach(Boston)
attach(Boston)
mean(medv)
sd(medv)/sqrt(dim(Boston[1]))
sd(medv)/sqrt(dim(Boston)[1])
t.test(medv)
?quantile
summary(lstat)
ols(medv~.)
lm(medv.~)
lm(medv~.)
lm(medv~., data=Boston)
lm.fit =lm(medv~., data=Boston)
summary(lm.fit)
detach(Boston)
library(MASS) # for the Boston's housing data
set.seed(1)
train = sample(dim(Boston)[1], dim(Boston)[1]/2)
rm(list=ls())
library(MASS) # for the Boston's housing data
set.seed(1)
train = sample(dim(Boston)[1], dim(Boston)[1]/2)
attach(Boston)
detach(Boston)
set.seed(1)
train = sample(dim(Boston)[1], dim(Boston)[1]/2)
lm.fit = lm(medv~., data = Boston, subset=train)
lm.pred = predict(lm.fit)
lm.pred = predict(lm.fit, Boston[-train])
lm.pred = predict(lm.fit, Boston[-train,]) #note that it's not !train since train is not logical here
lm.pred = predict(lm.fit, Boston[-train]) #note that it's not !train since train is not logical here
summary(lm.pred)
lm.pred = predict(lm.fit, Boston[-train,]) #note that it's not !train since train is not logical here
lm.pred = predict(lm.fit, Boston[-train]) #note that it's not !train since train is not logical here
lm.pred = predict(lm.fit, Boston[-train,]) #note that it's not !train since train is not logical here
mean((medv[-train]-lm.pred)^2)
rm(list=ls())
library(MASS) # for the Boston's housing data
library(boot) # for cv.glm function
glm.fit =glm(medv~., data=Boston) #without family argument, by default is OLS
cv.error = cv.glm(Boston, glm.fit)
cv.error$delta[1]
set.seed(1)
glm.fit =glm(medv~., data=Boston) #without family argument, by default is OLS
cv.error = cv.glm(Boston, glm.fit)
# prediction error
cv.error$delta[1]
rm(list=ls())
library(MASS) # for the Boston's housing data
library(boot) # for cv.glm function
set.seed(1)
glm.fit =glm(medv~., data=Boston) #without family argument, by default is OLS
cv.error = cv.glm(Boston, glm.fit, K=10)
# prediction error
cv.error$delta[1]
rm(list=ls())
library(MASS) # for the Boston's housing data
library(boot) # for the boot() function (bootstrap analysis)
boot.fn  = function(data, index)
lm(medv~., data=data, subset=index)$coefficients
boot(Boston, boot.fn, 100) #boostrap 100 times
lm(medv~.,Boston)
summary(lm(medv~.,Boston))
rm(list=ls())
library(MASS) # for the Boston's housing data
library(boot) # for cv.glm function
set.seed(23)
k.fold = function(data, degree) {
glm.fit = glm(medv~.poly(lstat, degree), data=data)
# prediction error
return(cv.glm(data, glm.fit, K=5))
}
for (degree in 1:10)
plot(degree, k.fold(Boston, degree))
?poly
k.fold = function(data, degree) {
glm.fit = glm(medv~poly(lstat, degree), data=data)
# prediction error
return(cv.glm(data, glm.fit, K=5))
}
for (degree in 1:10)
plot(degree, k.fold(Boston, degree))
k.fold = function(data, degree) {
glm.fit = glm(medv~poly(lstat, degree), data=data)
# prediction error
return(cv.glm(data, glm.fit, K=5)$delta[1])
}
for (degree in 1:10)
plot(degree, k.fold(Boston, degree))
for (degree in 1:10) {
plot(degree, k.fold(Boston, degree))
}
k.fold = function(data, degree) {
glm.fit = glm(medv~poly(lstat, degree), data=data)
# prediction error
cv.glm(data, glm.fit, K=5)$delta[1] #delta elements in the function cv.glm
}
for (degree in 1:10)
plot(degree, k.fold(Boston, degree))
for (degree in 1:10)
k.fold(Boston, degree)
for (degree in 1:10)
{k.fold(Boston, degree)  }
k.fold = function(degree) {
glm.fit = glm(medv~poly(lstat, degree), data=Boston)
# prediction error
cv.glm(Boston, glm.fit, K=5)$delta[1] #delta elements in the function cv.glm
}
for (degree in 1:10)
{k.fold(Boston, degree)  }
for (degree in 1:10)
{k.fold(degree)  }
k.fold(1)
k.fold(2)
k.fold = function(degree) {
glm.fit = glm(medv~poly(lstat, degree), data=Boston)
# prediction error
cv.glm(Boston, glm.fit, K=20)$delta[1] #delta elements in the function cv.glm
}
for (degree in 1:10)
k.fold(2)
k.fold(2)
for (degree in 1:10)
k.fold(degree)
for (degree in 1:10)
plot(degree, k.fold(degree))
?plot(degree, cv.error)
rm(list=ls())
library(MASS) # for the Boston's housing data
library(boot) # for cv.glm function
set.seed(23)
k.fold = function(data, degree) {
glm.fit = glm(medv~poly(lstat, degree), data=Boston)
# prediction error
cv.glm(Boston, glm.fit, K=20)$delta[1] #delta elements in the function cv.glm
}
cv.error = rep(0,20) # for 20-fold cv
for (degree in 1:8) # for polynomials to the power of 8
cv.error[degree] = k.fold(Boston, degree)
plot(degree, cv.error, type="b", col="turquoise") # b for both line and point drawings
cv.error = rep(0,10)
for (degree in 1:10) # for polynomials to the power of 8
cv.error[degree] = k.fold(Boston, degree)
plot(degree, cv.error, type="b", col="turquoise") # b for both line and point drawings
cv.error = rep(0,10)
degree = 1:10
for (d in degree) # for polynomials to the power of 8
cv.error[d] = k.fold(Boston, d)
for (d in degree) # for polynomials to the power of 10
cv.error[d] = k.fold(Boston, d)
plot(degree, cv.error, type="b", col="turquoise") # b for both line and point drawings
plot(degree, cv.error, type="b", col="maroon") # b for both line and point drawings
setwd("~/GitHub/wine-quality-tree.R")
wine = read.csv("wine.csv")
summary(wine)
fix(wine)
names(wine) = c("quality", "alcohol", "malic_acid", "ash","alcalinity_of_ash",
"magnesium", "total_phenols", "flavanoids", "nonflavanoid_phenols",
"proanthocyanins", "color_intensity", "hue", "OD280", "proline")
summary(wine)
setwd("~/GitHub/wine-quality-tree.R")
source("prep.R")
setwd("~/GitHub/wine-quality-tree.R")
source("prep.R")
tree.wine = tree(quality~., data=wine)
library(tree)
tree.wine = tree(quality~., data=wine)
summary(tree.wine)
?tree
class(wine$quality)
class(wine$quality)="factor"
class(wine$quality)
wine$quality = as.factor(wine$quality)
class(wine$quality)
summary(wine#=$quality)
}
summary(wine$quality)
is.na(wine$quality)
wine$quality
wine$quality
wine = read.csv("wine.csv")
names(wine) = c("quality", "alcohol", "malic_acid", "ash","alcalinity_of_ash",
"magnesium", "total_phenols", "flavanoids", "nonflavanoid_phenols",
"proanthocyanins", "color_intensity", "hue", "OD280", "proline")
summary(wine)
str(wine)
wine$quality = as.factor(wine$quality)
str(wine)
summary(wine)
source("prep.R")
library(tree)
tree.wine = tree(quality~., data=wine)
summary(tree.wine)
plot(tree.wine)
text(tree.wine)
tree.wine
train = sample(c(TRUE, FALSe), dim(wine)[1]/2, rep=TRUE)
train = sample(c(TRUE, FALSE), dim(wine)[1]/2, rep=TRUE)
set.seed(1)
train = sample(c(TRUE, FALSE), dim(wine)[1]/2, rep=TRUE)
tree.wine = tree(quality~., data=wine, subset=train)
pred = predict(tree.wine, wine[!train,], type="class")
with(wine[!train,],table(pred, wine$quality))
table(pred, wine$quality[!train])
mean(pred==wine$quality[!train])
cv.wine = cv.tree(tree.wine, FUN=prune.misclass)
names(cv.wine)
plot(cv.wine$size, cv.wine$dev)
cv.wine
plot(cv.wine$size, cv.wine$dev, type="l")
min(cv.wine$dev)
which.min(cv.wine$dev)
prune.wine = prune.misclass(tree.wine, best=3)
plot(prune.wine)
text(prune.wine)
pred = predict(prune.wine, wine[!train,], type="class")
table(pred, wine$quality[!train])
mean(pred==wine$quality[!train])
setwd("~/GitHub/wine-quality-tree.R")
source("prep.R")
library(randomForest)
train = sample(c(TRUE, FALSE), dim(wine)[1]/2, rep=T)
x.train = wine[train,-1]
y.train = wine[train,1]
x.test = wine[!train, -1]
y.test = wine[!train, 1]
?randomForest
bag1 = randomForest(x.train,y.train, x.test, y.test, ntree=500, mtry=m1, importance=T)
p = 13 # there are 13 predictors
# for the bagging method, mtry = p
m1 = p
m2 = p/2
m3 = sqr(p) # this is random forest method
bag1 = randomForest(x.train,y.train, x.test, y.test, ntree=500, mtry=m1, importance=T)
m3 = sqrt(p) # this is random forest method
bag1 = randomForest(x.train,y.train, x.test, y.test, ntree=500, mtry=m1, importance=T)
bag2 = randomForest(x.train,y.train, x.test, y.test, ntree=500, mtry=m2, importance=T)
bag3 = randomForest(x.train,y.train, x.test, y.test, ntree=500, mtry=m3, importance=T)
plot(1:500, bag1$test$mse, col= "red", type="l", xlab="Number of trees", ylab= "test MSE")
lines(1:500, bag2$test$mse, col="blue", type="l")
m1 = p
m2 = p/2
set.seed(23)
train = sample(c(TRUE, FALSE), dim(wine)[1]/2, rep=T)
x.train = wine[train,-1]
y.train = wine[train,1]
x.test = wine[!train, -1]
y.test = wine[!train, 1]
# here I am going to perform different mtry (splits) and compare the MSEs
?randomForest
p = 13 # there are 13 predictors
# for the bagging method, mtry = p
m1 = p
m2 = p/2
m2 = round(p/2)
m3 = round(sqrt(p)) # this is random forest method
bag1 = randomForest(x.train,y.train, x.test, y.test, ntree=500, mtry=m1, importance=T)
bag2 = randomForest(x.train,y.train, x.test, y.test, ntree=500, mtry=m2, importance=T)
bag3 = randomForest(x.train,y.train, x.test, y.test, ntree=500, mtry=m3, importance=T)
plot(1:500, bag1$test$mse, col= "red", type="l", xlab="Number of trees", ylab= "test MSE")
lines(1:500, bag2$test$mse, col="blue", type="l")
?randomForest
?randomForest
bag.wine = randomForest(quality~., data=wine, mtry=13, importance=T)
bag.wine
names(bag.wine)
names(bag.wine$test)
names(bag.wine$confusion)
importance(bag.wine)
rf.wine = randomForest(quality~., data=wine, mtry=4, importance=T)
rf.wine
plot(rf.wine)
plot(bag.wine)
varImpPlot(rf.wine)
setwd("~/GitHub/wine-quality-tree.R")
source("prep.R")
library(gbm)
set.seed(321)
train = sample(c(TRUE, FALSE), dim(wine)[1]/2, rep=TRUE)
n.trees = 5000, interaction.depth = 4)
n.trees = 5000, interaction.depth = 4)
boost.wine = gbm(quality~., data=wine[train,], distribution="bernoulli",
n.trees = 5000, interaction.depth = 4)
boost.wine = gbm(quality~., data=wine[train,], distribution = "multinomial"
n.trees = 5000, interaction.depth = 4)
n.trees = 5000, interaction.depth = 4)
boost.wine = gbm(quality~., data=wine[train,], distribution = "multinomial",
n.trees = 5000, interaction.depth = 4)
summary(boost.wine)
plot(boost.wine, i="color_intensity")
names(boost.wine)
boost.wine = gbm(quality~., data=wine, distribution = "multinomial",
n.trees = 5000, interaction.depth = 4)
summary(boost.wine)
plot(boost.wine, i="color_intensity")
names(boost.wine)
summary(boost.wine$cv.folds)
summary(boost.wine$fit)
set.seed(321)
train = sample(c(TRUE, FALSE), dim(wine)[1]/2, rep=TRUE)
# note that for classification problem, we shoudl address the distribution as "bernoulli"
# more than 2 classes, should be "multinomial"
# for the regression problem, I can address it "gaussian"
boost.wine = gbm(quality~., data=wine[train,], distribution = "multinomial",
n.trees = 5000, interaction.depth = 4)
summary(boost.wine)
plot(boost.wine, i="color_intensity")
yhat = predict(boost.wine, newdata=wine[!train,], n.trees=5000)
summary(yhat)
yhat = predict(boost.wine, newdata=wine[!train,], n.trees=5000, type="class")
yhat = predict(boost.wine, newdata=wine[!train,], n.trees=5000, type="link")
summary(yhat)
plot.gbm
?plot.gbm
?predict.gbm
summary(yhat)
yhat = predict(boost.wine, newdata=wine[!train,], n.trees=5000, type="response")
summary(yhat)
constrast(wine$quality)
constrasts(wine$quality)
contrasts(wine$quality)
predict(boost.wine, newdata=wine[!train,], n.trees=5000)
dim(pred)
pred = predict(boost.wine, newdata=wine[!train,], n.trees=5000)
dim(pred)
quality.pred = which.max(pred)
quality.pred
quality.pred = rep(N, dim(pred[1]))
quality.pred = which.max(pred[i,])
for (i in 1:dim(pred[1]))
quality.pred = which.max(pred[i,])
quality.pred = rep(N, dim(pred[1]))
for (i in 1:dim(pred[1]))
quality.pred = which.max(pred[i,])
quality.pred = rep(N, dim(pred)[1])
for (i in 1:dim(pred)[1])
quality.pred = which.max(pred[i,])
pred[2,]
pred
names(pred)
dim(pred)
class(pred)
quality.pred = rep(N, dim(pred)[1])
quality.pred = rep(NA, dim(pred)[1])
for (i in 1:dim(pred)[1])
quality.pred = which.max(pred[i,,])
quality.pred
pred[,,1]
pred[,,2]
pred = pred[,,1]
dim(pred)
for (i in 1:dim(pred)[1])
quality.pred = which.max(pred[i,])
quality.pred
which.max(pred[1,])
which.max(pred[2,])
which.max(pred[89,])
for (i in 1:dim(pred)[1])
quality.pred = which.max(pred[i,][1]) # because it gives out the same two values
quality.pred
quality.pred = rep(NA, dim(pred)[1])
for (i in 1:dim(pred)[1])
quality.pred = which.max(pred[i,][1]) # because it gives out the same two values
for (i in 1:dim(pred)[1])
quality.pred[i] = which.max(pred[i,][1]) # because it gives out the same two values
for (i in 1:dim(pred)[1])
quality.pred[i] = which.max(pred[i,]) # because it gives out the same two values
quality.pred
summary(quality.pred)
class.quality.pred
class(quality.pred)
